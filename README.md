# NLP-with-Simpsons

This dataset contains the characters, locations, episode details, and script lines for approximately 600 Simpsons episodes, dating back to 1989. I am having fun to aspply some technics with Simpsons.

Setting up the environment:
python==3.6.7

Libraries used:

* pandas
* numpy
* spacy
* nltk
* sklearn
* seaborn
* matplotlib

### So here used such technics as:
* Cleaning text
* Exploratory data analysis
* Tf Idf similarity
* Generating nGrams
* Word2Vec model
* Sentiment Analysis
* NLP trics with nltk module
* Visualization trics

<hr>

### Simsons.ipynb Agenda:

1. Reading/Cleaning/Pprocessing data set. 
2. Top most spoken characters.
![alt text](https://user-images.githubusercontent.com/10981310/70634495-fd34d380-1c3a-11ea-8b2b-ae5aaabe35ac.PNG)
3. Top most Common Words
![alt text](https://user-images.githubusercontent.com/10981310/70634494-fd34d380-1c3a-11ea-9559-767b5dc24154.PNG)
4. Words cloud
![alt text](https://user-images.githubusercontent.com/10981310/70634496-fd34d380-1c3a-11ea-98f1-2144c425fa1d.PNG)
5. Dispersion plot.
6. Top Most Important words for specified character. (TF-IDF)
7. Relationship among words (N-grams with network graph).
![alt text](https://user-images.githubusercontent.com/10981310/70634497-fd34d380-1c3a-11ea-94a5-5f8047499e6f.PNG)
8. Sentiment analysis.
![alt text](https://user-images.githubusercontent.com/10981310/70634492-fd34d380-1c3a-11ea-918a-c48a7719ffba.PNG)
9. Word2Vec model implementation.
![alt text](https://user-images.githubusercontent.com/10981310/70634489-fc9c3d00-1c3a-11ea-889d-2e68473d5521.PNG)
10. Word2Vec results interpretations and visualization.
