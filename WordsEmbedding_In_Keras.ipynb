{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd  # For data handling\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Input\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 500)\n",
    "pd.set_option(\"display.max_rows\", 999)\n",
    "pd.set_option('display.max_colwidth', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word embeddings provide a dense representation of words and their relative meanings.\n",
    "\n",
    "Here willl consider:\n",
    "\n",
    "* Words Embeddings via the Embedding layer.\n",
    "* How to learn a word embedding while fitting a neural network.\n",
    "\n",
    "<hr>\n",
    "\n",
    "* Word Embedding.\n",
    "* Keras Embedding Layer.\n",
    "* Example of Learning an Embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A word embedding is a class of approaches for representing words and documents using a dense vector representation.\n",
    "\n",
    "It is an improvement over more the traditional bag-of-word model.\n",
    "\n",
    "The position of a word within the vector space is learned from text and is based on the words that surround the word when it is used.\n",
    "\n",
    "\n",
    "### Keras Embedding Layer\n",
    "In keras we can find Embedding layer option, that can be used for neural networks on text data.\n",
    "\n",
    "For neccessary that the input data be integer encoded, so that each word is represented by a unique integer. This data preparation step can be performed using the Tokenizer API also provided with Keras, sklearn or nltk packages.\n",
    "\n",
    "The Embedding layer is initialized with random weights and will learn an embedding for all of the words in the training dataset.\n",
    "\n",
    "It is a flexible layer that can be used:\n",
    "\n",
    "* Alone to learn a word embedding that can be saved and used in another model later.\n",
    "* Part of a deep learning model where the embedding is learned along with the model itself.\n",
    "* To load a pre-trained word embedding model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keras Embedding Layer arguments:\n",
    "\n",
    "**input_dim**: This is the size of the vocabulary in the text data. For example, if your data is integer encoded to values between 0-10, then the size of the vocabulary would be 11 words.\n",
    "\n",
    "**output_dim**: This is the size of the vector space in which words will be embedded. It defines the size of the output vectors from this layer for each word. For example, it could be 32 or 100 or even larger. Test different values for your problem.\n",
    "\n",
    "**input_length**: This is the length of input sequences, as you would define for any input layer of a Keras model. For example, if all of your input documents are comprised of 1000 words, this would be 1000.\n",
    "For example, below we define an Embedding layer with a vocabulary of 200 (e.g. integer encoded words from 0 to 199, inclusive), a vector space of 32 dimensions in which words will be embedded, and input documents that have 50 words each.\n",
    "\n",
    "For example, below we define an Embedding layer with a vocabulary of 200 (e.g. integer encoded words from 0 to 199, inclusive), a vector space of 32 dimensions in which words will be embedded, and input documents that have 50 words each.\n",
    "\n",
    "e = Embedding(200, 32, input_length=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of Learning an Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define documents\n",
    "docs = ['Well done',\n",
    "        'Good work',\n",
    "        'Great effort',\n",
    "        'nice work',\n",
    "        'Excellent!',\n",
    "        'Weak',\n",
    "        'Poor effort',\n",
    "        'not good',\n",
    "        'poor work',\n",
    "        'Could have done better.']\n",
    "# define class labels\n",
    "labels = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer encode document \n",
    "\n",
    "Using Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 14 unique tokens.\n",
      "Shape of data tensor: (10, 4)\n",
      "[[ 6  2  0  0]\n",
      " [ 3  1  0  0]\n",
      " [ 7  4  0  0]\n",
      " [ 8  1  0  0]\n",
      " [ 9  0  0  0]\n",
      " [10  0  0  0]\n",
      " [ 5  4  0  0]\n",
      " [11  3  0  0]\n",
      " [ 5  1  0  0]\n",
      " [12 13  2 14]]\n"
     ]
    }
   ],
   "source": [
    "### Integer encode document using Tokenizer\n",
    "vocab_size = 20\n",
    "tokenizer = Tokenizer(vocab_size)\n",
    "tokenizer.fit_on_texts(docs)\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(docs)\n",
    "word_index = tokenizer.word_index\n",
    "input_dim = len(word_index) + 1\n",
    "\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(sequences, max_length, padding='post')\n",
    "print('Shape of data tensor:', padded_docs.shape)\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integer encode document \n",
    "\n",
    "Using Keras one_hot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (10, 4)\n",
      "[[17  5  0  0]\n",
      " [ 3 10  0  0]\n",
      " [14 18  0  0]\n",
      " [ 8 10  0  0]\n",
      " [ 1  0  0  0]\n",
      " [ 9  0  0  0]\n",
      " [ 5 18  0  0]\n",
      " [17  3  0  0]\n",
      " [ 5 10  0  0]\n",
      " [14  1  5 11]]\n"
     ]
    }
   ],
   "source": [
    "### Integer encode document using Keras one_hot()\n",
    "# Keras provides the one_hot() function that creates a hash of each word as an efficient integer encoding. \n",
    "# We will estimate the vocabulary size of 50, which is much larger than needed to reduce the probability of collisions from the hash function.\n",
    "vocab_size = 20\n",
    "encoded_docs = [one_hot(d, vocab_size) for d in docs]\n",
    "\n",
    "# The sequences have different lengths and Keras prefers inputs to be vectorized and all inputs to have the same length. \n",
    "# We will pad all input sequences to have the length of max vector = 4. \n",
    "max_length = 4\n",
    "padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')\n",
    "print('Shape of data tensor:', padded_docs.shape)\n",
    "print(padded_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now ready to define our Embedding layer.\n",
    "\n",
    "The Embedding has a vocabulary of 50 and an input length of 4. We will choose a small embedding space of 8 dimensions.\n",
    "\n",
    "The model is binary classification model. \n",
    "\n",
    "Output from the Embedding layer will be 4 vectors of 8 dimensions each, one for each word. \n",
    "\n",
    "We flatten this to a one 32-element vector to pass on to the Dense output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_29 (Embedding)     (None, 4, 4)              80        \n",
      "_________________________________________________________________\n",
      "flatten_24 (Flatten)         (None, 16)                0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 97\n",
      "Trainable params: 97\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 4, input_length=max_length))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# summarize the model\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.000000\n",
      "Loss: 0.162495\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(padded_docs, labels, epochs=50, verbose=0)\n",
    "# evaluate the model\n",
    "loss, accuracy = model.evaluate(padded_docs, labels, verbose=0)\n",
    "print('Accuracy: %f' % (accuracy*100))\n",
    "print('Loss: %f' % (loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings :  (20, 4)\n",
      "\n",
      "Embedings:\n",
      "\n",
      "[[-1.99764460e-01  2.29839504e-01  9.99004692e-02  2.30218112e-01]\n",
      " [-5.64353406e-01  5.74807465e-01  5.72019935e-01 -3.64473052e-02]\n",
      " [-1.15214810e-02 -4.02787551e-02 -2.76461728e-02 -4.48251888e-03]\n",
      " [-5.80309570e-01  5.91365814e-01  6.23466253e-01 -4.25502479e-01]\n",
      " [-4.70023640e-02 -7.97265768e-03  4.11068909e-02 -3.20850238e-02]\n",
      " [ 4.74460661e-01 -5.13310313e-01 -7.26960421e-01 -6.04268193e-01]\n",
      " [-2.04268694e-02  8.55582952e-03  4.96452712e-02  5.87750226e-04]\n",
      " [-1.81722641e-03 -4.38152552e-02  3.40045616e-03 -3.73443738e-02]\n",
      " [-4.37325537e-01  4.21925187e-01  2.19033450e-01  4.69510376e-01]\n",
      " [ 5.12585521e-01 -4.97423023e-01 -3.36204201e-01 -5.36920726e-01]\n",
      " [ 2.94415832e-01  1.05284825e-01 -1.90992221e-01  2.26864070e-01]\n",
      " [-5.49828410e-01 -4.33199018e-01  4.41136420e-01 -4.30477470e-01]\n",
      " [-2.78783962e-03  1.16887689e-02 -1.26105435e-02 -2.61036046e-02]\n",
      " [-1.07128546e-03  6.29330799e-03  1.53057836e-02  6.87025860e-03]\n",
      " [-6.82420611e-01  5.91403008e-01  4.58785594e-01  6.55931652e-01]\n",
      " [ 1.43297426e-02 -2.13685874e-02 -9.51081514e-03 -3.70822661e-02]\n",
      " [-4.35739271e-02  2.14486197e-03 -4.34728637e-02  4.89335917e-02]\n",
      " [ 2.26599118e-03  6.28467500e-02  3.77911851e-02  2.26150397e-02]\n",
      " [-2.41832793e-01 -4.14503403e-02  1.43197209e-01 -1.43188313e-01]\n",
      " [ 1.65635385e-02  1.44395977e-03  6.21439144e-03  3.11218537e-02]]\n"
     ]
    }
   ],
   "source": [
    "wEmb = model.layers[0].get_weights()[0]\n",
    "print(\"Shape of embeddings : \", wEmb.shape) \n",
    "print(\"\\nEmbedings:\\n\")\n",
    "print(wEmb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforSentence(sentences, vocab_size, max_length):\n",
    "    v = [one_hot(d, vocab_size) for d in sentences]\n",
    "    v = pad_sequences(v, maxlen=max_length, padding='post')\n",
    "    return v\n",
    "\n",
    "sentences = [\"Well done\"]\n",
    "k = transforSentence(sentences=sentences, vocab_size=vocab_size, max_length=max_length)\n",
    "\n",
    "pred = model.predict(k)\n",
    "print(\"Sentence: \" + str(sentences) + \"  Prediction: \" + str(pred[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['Poor work']  Prediction: [0.4878812]\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"Poor work\"]\n",
    "k = transforSentence(sentences=sentences, vocab_size=vocab_size, max_length=max_length)\n",
    "\n",
    "pred = model.predict(k)\n",
    "print(\"Sentence: \" + str(sentences) + \"  Prediction: \" + str(pred[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## How looks word embedings?\n",
    "\n",
    "Exctracting word vectors from Keras Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings :  (10, 8)\n"
     ]
    }
   ],
   "source": [
    "### How looks word embedings\n",
    "\n",
    "output_dim = 2\n",
    "model2 = Sequential()\n",
    "model2.add(Embedding(vocab_size, output_dim, input_length=max_length))\n",
    "model2.add(Flatten())\n",
    "# compile the model\n",
    "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "embeddings = model2.predict(padded_docs) # finally getting the embeddings.\n",
    "print(\"Shape of embeddings : \", embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings :  (10, 4, 2)\n",
      "\n",
      "Embedings:\n",
      "\n",
      "[[-0.00379983  0.0309325  -0.03431519 -0.01433668  0.03334451 -0.01893507\n",
      "   0.03334451 -0.01893507]\n",
      " [ 0.04388897  0.00719521 -0.03590513  0.03512326  0.03334451 -0.01893507\n",
      "   0.03334451 -0.01893507]\n",
      " [ 0.02390401  0.00492169 -0.01700457  0.00464555  0.03334451 -0.01893507\n",
      "   0.03334451 -0.01893507]\n",
      " [ 0.03399228 -0.0267233  -0.03590513  0.03512326  0.03334451 -0.01893507\n",
      "   0.03334451 -0.01893507]\n",
      " [-0.01990628  0.03737198  0.03334451 -0.01893507  0.03334451 -0.01893507\n",
      "   0.03334451 -0.01893507]\n",
      " [-0.01431173  0.0475159   0.03334451 -0.01893507  0.03334451 -0.01893507\n",
      "   0.03334451 -0.01893507]\n",
      " [-0.03431519 -0.01433668 -0.01700457  0.00464555  0.03334451 -0.01893507\n",
      "   0.03334451 -0.01893507]\n",
      " [-0.00379983  0.0309325   0.04388897  0.00719521  0.03334451 -0.01893507\n",
      "   0.03334451 -0.01893507]\n",
      " [-0.03431519 -0.01433668 -0.03590513  0.03512326  0.03334451 -0.01893507\n",
      "   0.03334451 -0.01893507]\n",
      " [ 0.02390401  0.00492169 -0.01990628  0.03737198 -0.03431519 -0.01433668\n",
      "  -0.01014149 -0.02332916]]\n"
     ]
    }
   ],
   "source": [
    "embeddings2 = embeddings.reshape(-1, max_length, output_dim)\n",
    "print(\"Shape of embeddings : \", embeddings2.shape) \n",
    "print(\"\\nEmbedings:\\n\")\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 ---> number of documents\n",
    "\n",
    "4  ---> each document is made of 4 words which was our maximum length of any document.\n",
    "\n",
    "2  ---> each word is 2 dimensional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model2.layers[0].get_weights()[0]\n",
    "words_embeddings = {w:embeddings[idx] for w, idx in word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'work': array([-0.01990628,  0.03737198], dtype=float32),\n",
       " 'done': array([-0.00952191, -0.00654057], dtype=float32),\n",
       " 'good': array([0.04388897, 0.00719521], dtype=float32),\n",
       " 'effort': array([0.02222503, 0.02809996], dtype=float32),\n",
       " 'poor': array([-0.03431519, -0.01433668], dtype=float32),\n",
       " 'well': array([ 0.04503355, -0.04771587], dtype=float32),\n",
       " 'great': array([ 3.0855570e-02, -2.4296343e-05], dtype=float32),\n",
       " 'nice': array([ 0.03399228, -0.0267233 ], dtype=float32),\n",
       " 'excellent': array([-0.01431173,  0.0475159 ], dtype=float32),\n",
       " 'weak': array([-0.03590513,  0.03512326], dtype=float32),\n",
       " 'not': array([-0.01014149, -0.02332916], dtype=float32),\n",
       " 'could': array([-0.03128161, -0.00627512], dtype=float32),\n",
       " 'have': array([ 0.01552765, -0.03073349], dtype=float32),\n",
       " 'better': array([0.02390401, 0.00492169], dtype=float32)}"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:python3] *",
   "language": "python",
   "name": "conda-env-python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
